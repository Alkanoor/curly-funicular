Bien qu'il soit préférable d'éviter les approches rasoirs lorsqu'on souhaite introduire un sujet quelconque, on ne peut pas dans le cas de cette étude portant en partie sur les réseaux SDN, oublier de mentionner quelques éléments difficilement contestables sur les réseaux actuels \footnote{\label{of_def}Aussi résumé dans \url{https://www.opennetworking.org/sdn-resources/sdn-definition}} :\\

\begin{list}{$\Asteriscus$}{}

\item La demande ne cesse de croître : on observe un accroissement considérable des enjeux liés au traitement de masse importante de données, de l'utilisation de services cloud, du trafic mobile et peut être bientôt de l'utilisation d'objets connectés. Or tous ces éléments présentent le point commun de communiquer avec de nombreuses entités situées sur des réseaux potentiellement éloignés. Cela mobilise donc un trafic réseau intense.

\item Les technologies actuelles pour soutenir cette demande énorme sont capables de fournir un débit titanesque : que l'on considère des technologies sans fil ou non, au coeur des réseaux tant au niveau des terminaux des utilisateurs, on atteint aujourd'hui des débits théorique de l'ordre du Gigabit par seconde pour l'utilisateur. Tout cela sans que l'on ait vraiment conscience des conditions que cela requiert.

\item Les méthodes d'accès sont aujourd'hui bien différentes. Précédemment le modèle client/serveur était largement employé, avec dans le cas d'une entreprise, un réseau interne constitué de plusieurs LAN séparés, et connecté à internet de manière quasiment unique. Cela entrainant une configuration possiblement statique et donc aisée, le trafic se déroulant principalement sur un mode requête/réponse. Or la tendance, notamment à cause des deux premiers points, est à l'émergence de nouveaux modes d'accès plus horizontaux. Ce type de communication tient entre autres de la distribution plus éparse des données à travers le réseau : grossissement de la taille des bases de données, duplication de celles-ci (mise en cache sur différents serveur à travers le monde pour permettre un accès plus rapide), augmentation du trafic volumineux (vidéo, voix) et de nouveaux trafics (IoT (Bring Your Own Device), ...) même au sein de l'entreprise. Enfin, l'utilisation de plus en plus répandue de services cloud, avec ses implications en terme de virtualisation (que ce soit des applications, ou bien des bases de données), susceptible de changer en permanence la localisation des serveurs pour garantir une certaine flexibilité.\\

\end{list}

Or, le réseau principal global tel que nous le connaissons (la partie reposant sur TCP/IP en tout cas) a été conçu d'abord dans un but de résilience : chaque paquet doit être reçu, et peu importe la route empruntée. L'architecture distribuée actuelle n'est donc pas batie pour assurer spécifiquement extensibilité, ni qualité de service définie. Le routeur (et le réseau d'ailleurs) des années 1980 a donc été progressivement amélioré sur la base de ce paradigme initial, avec le plan de données et le plan de contrôle attachés aux mêmes équipements, configurés en partie manuellement. Tout s'est complexifié également : nouveaux protocoles, ajouts d'équipements capables de répartir la charge réseau, filtrer les paquets, prévenir de certaines tentatives d'attaque, etc ...\\
Certains éléments de réflexion peuvent éventuellement nous mettre sur la voie d'une complexité qui, à défaut d'être exponentielle, l'est d'avantage que simplement linéaire (c'est du moins une conviction personnelle non vérifiée) :

\begin{list}{$\Asteriscus$}{}
\item Plus il y a d'éléments statiques dans un réseau, et plus la modification en profondeur de celui-ci est coûteuse (puisqu'il faut penser à chaque impact sur les parties statiques).
\item Si un problème survient, dans le même ordre d'idée, il est difficile d'avoir une vue globale de ce qui se passe puisqu'aucune vue globale du réseau n'est accessible : il faut vérifier (potentiellement) que chaque élément se comporte correctement et est bien configuré.
\item Les interfaces entre switchs, routeurs et autres éléments peuvent varier selon le constructeur, et le logiciel sur les équipements est souvent propriétaire et complexe.
\end{list}

~\\
De nombreux problèmes se résolvent avec un niveau d'abstraction supplémentaire \footnote{\label{indirection}\url{https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering}}. Si le développement de systèmes de plus en plus complexes s'est fait de manière très rapide sur PC, c'est d'abord grâce à la première couche d'abstraction qu'ont concstituées les instructions assembleur, puis à la seconde qu'a été le système d'exploitation. Certaines personnes ont eu l'idée, au lieu de considérer le réseau comme un élément périphérique, de le voir comme un processeur capable d'exécuter des instructions basiques, fournissant de fait un service plus facilement adaptable. C'est sur ce principe que repose le Software Defined Network (SDN). Avec une couche d'abstraction supplémentaire que constitue le protocole choisi pour véhiculer le flot d'instructions (Openflow dans notre cas, mais il y en a d'autres que nous évoquerons succintement plus tard), et un système d'exploitation spécifique (Network Operating System, NOS), l'idée est de découpler les différents chemins qu'empruntent les données et le plan de contrôle, à la manière d'un système d'exploitation qui sépare le code d'un programme et les données qu'il utilise.