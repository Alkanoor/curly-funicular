On ne peut pas entamer cette étude portant en partie sur les réseaux SDN sans oublier de mentionner quelques éléments difficilement contestables sur les réseaux actuels \footnote{\label{of_def}Aussi résumé dans \url{https://www.opennetworking.org/sdn-resources/sdn-definition}} :

\begin{itemize}

\item La demande ne cesse de croître : on observe un accroissement considérable des enjeux liés au traitement de masse importante de données, de l'utilisation de services cloud, du trafic mobile et peut être bientôt de l'utilisation d'objets connectés. Or tous ces éléments présentent le point commun de communiquer avec de nombreuses entités situées sur des réseaux potentiellement éloignés. Cela mobilise donc un trafic réseau intense.

\item Les technologies actuelles pour soutenir cette demande énorme sont capables de fournir un débit titanesque : que l'on considère des technologies sans fil ou non, au coeur des réseaux tant au niveau des terminaux des utilisateurs, on atteint aujourd'hui des débits théoriques de l'ordre du Gigabit par seconde pour l'utilisateur. Tout cela sans que l'on ait vraiment conscience des conditions que cela requiert.

\item Les méthodes d'accès sont aujourd'hui bien différentes. Précédemment le modèle client/ serveur était largement employé, avec dans le cas d'une entreprise, un réseau interne constitué de plusieurs LAN séparés, et connecté à internet de manière quasiment unique. Cela entraînant une configuration possiblement statique et donc aisée, les échanges se déroulant principalement sur un mode requête/réponse. Or la tendance, notamment à cause des deux premiers points, est à l'émergence de nouveaux modes d'accès plus horizontaux (avec d'avantage d'entités faisant circuler l'information au même niveau "hiérarchique"). Ce type de communication tient entre autres de la distribution plus éparse des données à travers le réseau due au grossissement de la taille des bases de données, à la duplication de celles-ci (mise en cache sur différents serveur à travers le monde pour permettre un accès plus rapide), à l'augmentation du trafic volumineux (vidéo, voix) et de nouveaux trafics (IoT, Bring Your Own Device, ...) même au sein de l'entreprise. Enfin, l'utilisation de plus en plus répandue de services cloud, avec ses implications au niveau de la virtualisation (que ce soit des applications, ou bien des bases de données), susceptible de changer en permanence la localisation des serveurs pour garantir une certaine flexibilité.

\end{itemize}

Or, le réseau principal global tel que nous le connaissons (la partie reposant sur TCP/IP en tout cas) a été conçu d'abord dans un but de fiabilité : chaque paquet doit être reçu, peu importe la route empruntée. L'architecture distribuée actuelle n'est donc pas bâtie pour assurer spécifiquement une extension aisée des services fournis, ni une qualité de service définie. Le routeur (et le réseau d'ailleurs) des années 1980 a donc été progressivement amélioré sur la base de ce paradigme initial, avec le plan de données et le plan de contrôle attachés aux mêmes équipements, configurés en partie manuellement. Tout s'est complexifié également : nouveaux protocoles, ajouts d'équipements spécifiques (capables de répartir la charge réseau, de filtrer les paquets, de prévenir de certaines tentatives d'attaque, etc ...), ...\\
Certains éléments de réflexion peuvent éventuellement nous mettre sur la voie d'une complexité qui, à défaut d'être exponentielle, l'est d'avantage que simplement linéaire (c'est du moins une conviction personnelle non vérifiée) :

\begin{itemize}
\item Plus il y a d'éléments statiques dans un réseau, et plus la propagation des modifications d'ensemble est coûteuse (puisqu'il faut penser à chaque impact sur les parties statiques et modifier un à un chaque équipement).
\item Si un problème survient, il est difficile d'avoir une vue globale de ce qui se passe puisqu'à moins de disposer d'équipements spéciaux aucune vue globale du réseau n'est accessible : il faut vérifier (potentiellement) que chaque élément se comporte correctement et est bien configuré.
\item Les interfaces entre switchs, routeurs et autres éléments peuvent varier selon le constructeur, et le logiciel sur les équipements est souvent propriétaire et complexe, surtout dans le cas de gros réseaux hétérogènes, ce qui ne facilite pas forcément la bonne marche de l'ensemble.
\end{itemize}

De nombreux problèmes se résolvent avec un niveau d'abstraction supplémentaire \footnote{\label{indirection}\url{https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering}}. Si le dévelop-pement de systèmes de plus en plus complexes s'est fait de manière très rapide sur PC, c'est d'abord grâce à la première couche d'abstraction qu'ont constitué les instructions assembleur, puis à la seconde qu'a été le système d'exploitation. Certaines personnes ont eu l'idée, au lieu de considérer le réseau comme un élément périphérique, de le voir comme un processeur capable d'exécuter des instructions basiques, fournissant de fait un service plus facilement adaptable. C'est sur ce principe que repose le Software Defined Network (SDN). Avec une couche d'abstraction supplémentaire que constitue le protocole choisi pour véhiculer le flot d'instructions (Openflow dans notre cas, mais il y en a d'autres que nous évoquerons succintement plus tard), et un système d'exploitation spécifique (Network Operating System, NOS), l'idée est de découpler les différents chemins qu'empruntent les données et le plan de contrôle, à la manière d'un système d'exploitation qui sépare le code d'un programme et les données qu'il utilise.